# 与最大似然估计的联系

考虑这样一种情况，我们知道条件独立随机变量(测量值)$y_{1:T}=\{y_1,\cdots,y_T\}$的条件分布$p(y_k|\theta)$，但是参数$\theta \in \mathbb{R}^d$是未知的。估计参数的经典统计方法是最大似然法，其中我们最大化观测的联合概率分布，也被称作似然函数：
$$
\mathcal{L}(\theta) = \prod_{k=1}^T p(y_k|\theta)
$$
关于$\theta$的似然函数的最大值给出了最大似然估计(ML-estimate)：
$$
\hat{\theta} = \arg\max_{\theta}\mathcal{L}(\theta)
$$
贝叶斯推理和极大似然估计的不同是贝叶斯推理的起点是认为参数$\theta$是一个随机变量。那么参数$\theta$的后验分布可以使用贝叶斯规则来计算
$$
p(\theta|y_{1:T}) = \frac{p(y_{1:T}|\theta)p(\theta)}{p(y_{1:T})}
$$
其中$p(\theta)$是先验分布，它在我们看到任何数据之前对参数的先验进行建模；$p(y_{1:T})$是独立于参数$\theta$的归一化参数。归一化常数经常被忽略并且如果观测$y_{1:T}$在给定$\theta$时是条件独立的，那么参数的后验分布可以写为：
$$
p(\theta|y_{1:T})\propto p(\theta)\prod_{k=1}^T p(y_k|\theta)
$$
因为我们处理的是一个分布，所以我们现在可能会选择随机变量的最可能值，即由后验分布的最大值给出的最大后验概率(MAP)估计。均方意义下最优估计是参数后验均值(MMSE-estimate)。最大似然估计可以被看作是先验分布$p(\theta)\propto 1$的最大后验估计。

我们还可以将贝叶斯推理解释为将正则化项纳入最大似然估计的一种方便方法。然而，贝叶斯推理的这种正则化解释是相当有限的，因为贝叶斯推理远不止于此。

